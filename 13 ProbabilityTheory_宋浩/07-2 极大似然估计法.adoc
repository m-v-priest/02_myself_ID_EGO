

= 极大似然估计法
:sectnums:
:toclevels: 3
:toc: left

---

在统计领域，有两种对立的思想学派："贝叶斯学派"和"经典学派"（也称"频率学派"）. 他们之间最重要的区别就是: 如何看待被估计的"未知参数": +
-> 贝叶斯学派的观点是: 将其看成是已知分布的"随机变量". +
-> 而经典学派的观点是: 将其看成未知的待估计的"常量".

比如, 我们要通过一个物理试验, 来测量某个粒子的质量. +
-> 从经典学派的观点来看，虽然粒子的质量未知，但他本质上是一个确定的常数，不能将其看成是一个随机变量。 +
-> 而贝叶斯学派则截然不同，会将待估计的粒子质量, 看做是一个随机变量. 并利用人们对该粒子的已有的认知, 给它一个"先验分布"，按照分布的概率模型，使其集中在某个指定的范围中。

---

== 极大似然估计法 maximum likelihood estimation method

.标题
====
例如： +
image:img/0774.png[,]
====

有了这个例子，我们就能开始介绍"极大似然估计方法"。

首先看"离散型"的情形，*随机变量 X 的概率分布已知，但是这个分布的参数, 是未知的，需要我们去估计，我们把它记作是 θ.* 好比上面抛掷硬币的试验中，硬币正面朝上的概率是未知的，需要我们去估计，那么此时 θ 就代表了这个"待估计的"正面向上的概率值。

image:img/0775.png[,600]

image:img/0776.png[,]

image:img/0777.png[,]

换言之, *我们要取到"能令 stem:[ L(x_1,...,x_n; θ)] 的输出值(其输出值是个概率值)最大" 的那个θ值.* 这个θ值, 能令我们最可能得到"多次试验中所得到的 stem:[ x_i]的实际观测值".

image:img/0778.png[,]

image:img/0779.png[,]

image:img/0780.png[,]

.标题
====
例如： +
image:img/0781.png[,]
====


.标题
====
例如： +
image:img/0782.png[,]
====


.标题
====
例如： +
image:img/0783.png[,]

image:img/0784.png[,]
====




---


它是建立在"极大似然原理"的基础上的一个统计方法.

"极大似然原理"的直观想法是：一个随机试验如有若干个可能的结果A，B，C，…。若在一次试验中，结果A出现，则一般认为试验条件对A出现有利，也即A出现的概率很大。 **即: 概率大的事件, 比概率小的事件, 更容易发生. 我们就拿样本中"使事件A发生的概率, 最大的那个参数值", 来作为总体中的该参数的估计值.**

最大似然估计，只是一种概率论在统计学的应用，它是参数估计的方法之一。说的是: 已知某个随机样本满足某种概率分布，但是其中具体的参数不清楚，"参数估计"就是通过若干次试验，观察其结果，利用结果推出参数的大概值。

*"最大似然估计"是建立在这样的思想上：已知某个参数能使这个样本出现的概率最大，我们当然不会再去选择其他小概率的样本，所以干脆就把这个参数, 作为估计的真实值。*



.标题
====
例如： +
image:img/0773.png[,800]
====


.标题
====
例如： +
image:img/0785.png[,700]
====




.标题
====
例如： +
image:img/0786.png[,700]
====


.标题
====
例如： +
image:img/0787.png[,750]
====

---


