
= 大数定律
:sectnums:
:toclevels: 3
:toc: left

---

== 大数定律 Law of Large Numbers

=== 解释1 (刘嘉概率讲座)

*在数据无限的情况下, 随机事件发生的频率, 一定会等于它的概率. 这就是"大数定律".* 但是, 现实中哪有什么无限呢? 所以当数据量很小的时候, 一件事发生的频率, 就和它的真实概率, 可能会相差很大.

协和飞机在被停飞之前, 只有区区8万次飞行数据 (而波音737飞行数据就超过1亿次), 所以我们永远不会知道它出事故的频率, 和它的真实事故概率之间, 到底会有多大的误差.

**很多人会有一种朴素的想法, 叫"补偿思维". 比如, 硬币连续抛10次都是正面后, 很多人就会认为, 下一次反面的概率肯定会更高一些, 因为这样才能补偿不平衡的状态, 让概率回到50%.  其实, 这种思维是错误的. 事实上: 整体不需要通过"补偿"来纠偏, 大数定律本身就自带"纠偏"功能. **

比如, 前10次抛硬币都是正面, 正面频率是 stem:[ 10/10 ], 我们继续抛下去, 到1000次, 正面的频率就变成了 stem:[ 510/1000=50.5%]; 继续抛下去, 到1万次, 正面频率就变成了 stem:[ 5100/10000=50.05%], 已经非常接近于50%了. *你发现了吗? 后面更大量数据的"回归均值"规律(大数定律), 会让概率重新回到真实概率上, 冲淡了数据较少时候时的概率偏差.*

*所以, 大数定律不会对已经发生的情况进行补偿,而是利用大量的正常数据, 来削弱那部分异常数据的影响。正常数据越多,异常数据的影响就越小，直到小到可以忽略不计。*

那么, 我们怎么保证, 未来一定会有大量的正常数据存在呢? 这是由"均值回归"所保证的.

**"均值回归(取均值回归)"的意思是就是说, 如果一个数据和它的正常状态, 偏差很大，那么它向正常状态回归的概率, 就会变大. 即, 趋向"均值方向"来回归.**

生活中的例子有很多, 如:

- 身高特别高的人, 孩子往往不如他高.
- 连续几年超高收益率的基金经理, 后几年往往神奇不在.

异常的的状态是没法长期持续的，所以回归正常值的概率会变大.  但, 回归后的数据, 是比正常值高一些，还是低一些，都有可能, 完全是随机的. 所以, **老话说, "三十年河东,三十年河西", "否极泰来", 都只说对了一半. 说对是指它们的确反映了"均值回归"规律. 说错是指它们带有错误的"补偿思维". **"大数定律"不需要通过"补偿"来实现. 所以极度的坏运气后, 不一定就有好运气, "均值回归"只是让运气回到不那么坏的正常状态.







---

=== 解释2

大数, 即"大量重复试验" 它们的平均结果 (主要指"期望"), 具有稳定性.

大数定理简单来说，指得是: 某个随机事件在单次试验中可能发生也可能不发生，但在大量重复实验中, 往往呈现出明显的规律性，即该随机事件发生的频率, 会向某个"常数值"收敛，该"常数值"即为该事件发生的概率。

另一种表达方式为: *当样本数据无限大时，"样本均值"趋于"总体均值".*

因为现实生活中，我们无法进行无穷多次试验，也很难估计出总体的参数。大数定律告诉我们, 能用"频率"近似代替"概率"；能用"样本均值"近似代替"总体均值". 这个定律就帮我们很好得解决了现实问题。

大数定理严格的数学定义分为两种，一是**"弱大数定理"：即"样本均值"会随着n的不断增大，"依概率收敛"**（简称i.p.收敛 converge in probability,）到真正的总体平均值。

*什么叫"依概率收敛"呢？意思是，当n越来越大时，随机变量x落在 (c-ε, c+ε) 外的概率, 趋近于0. 即还是有可能落在外面的，只不过可能是很小，且会随着n的增大，这种可能越来越小。*

二是**"强大数定理"：**我们仍可这样定义，*只不过这里不再是"依概率收敛"，而是几乎必然收敛*(简称a.s.收敛converge almost surely)，*可以理解为此时 p=1，以确定的为1的概率收敛，即没有x会落在
（c−ε，c+ε） 外面，就算有，这些点在测度下也是可以“忽略”的。*

*弱大数定律证明了：随着n的增大，"平均值"接近"真实期望值"的可能性也在增大。* +
*强大数定律证明：随着n的增大，"平均值"基本上就接近"真实期望值"了。*







---

== 依概率收敛

image:img/0432.png[,]

*记住: ε 是一个极小的正数 (无论它多么小). 相当于说, ε是一个无穷小的数.*

*"两个数的差"的绝对值, 意思就是这两个数的距离! 即它们相隔有多远*

---


== 伯努利大数定律

image:img/0436.png[,]

所谓"频率", 是指: 在相同的条件下，进行了n次试验，在这n次试验中，事件A发生的次数m, 就称为事件A发生的频数。比值 stem:[ m/n] 称为事件A发生的频率。

比如, 你射击的理论命中概率, 是0.9: +
10次射击, 打中8次, 频率就是 8/10 +
100次射击, 打中92次, 频率就是 92/100 +
1000次射击, 打中916次, 频率就是 916/1000 +
随着你射击次数(即 重复试验次数n)的增加 (n -> ∞), 大数定律就会显示出其作用, 你的命中"频率"会越来越接近与你的实际命中"概率".

但我们实际生活中, 没有人能对一件事重复很多次 (比如高考, 考公等), 更谈不上 n -> ∞次, 所以, "大数定律"就不会展现在你身上. 你得到的结果, 就可能是和你的"均值, 期望值μ" 会偏离很远的.


image:img/0439.png[,]

该定律其实是"切比雪夫大数定律"的特例，其含义是，当n足够大时，事件A出现的"频率", 将几乎接近于其发生的"概率"，即频率的稳定性。

image:img/0445.png[,]


---

== 切比雪夫不等式 chebyshev's theorem



image:img/0427.svg[,]

image:img/0428.png[,]


.标题
====
例如： +
image:img/0429.png[,]

image:img/0430.svg[,]
====


.标题
====
例如： +
image:img/0431.png[,]

====

---

== 切比雪夫大数定律

*"切比雪夫大数定律"是指，假设存在 n个相互独立的随机变量，当n 趋近于无穷时，这n个随机变量的"平均值", 也会趋近于这n个随机变量"期望"的"平均值".*

切比雪夫大数定律, 相比起一般我们听到的大数定律更一般，不仅能够解释"独立,同分布"随机变量的大数定律，也能够解释"独立,但不同分布"随机变量的大数定律。

image:img/0433.png[,]

image:img/0434.png[,]


image:img/0446.png[,]


---

== 辛钦大数定律

image:img/0435.png[,]

"辛钦大数定律", 和"切比雪夫大数定律"的区别是, 前者没有提到 stem:[ σ^2].

image:img/0447.png[,]

即, 这个定律告诉我们, 多次测求得的"平均数",可以逼近于"期望".


---
















